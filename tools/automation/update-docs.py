import argparse
import asyncio
import hashlib
import json
import logging
import os
import re
import sys
import urllib.request
from datetime import datetime
from pathlib import Path

import httpx

# [Configuration]
USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/133.0.0.0 Safari/537.36"
READER_API_PREFIX = "https://r.jina.ai/"


def setup_logging(log_dir: Path) -> logging.Logger:
    """íŒŒì¼ + ì½˜ì†” ë™ì‹œ ì¶œë ¥ ë¡œê±° ì´ˆê¸°í™”"""
    log_dir.mkdir(parents=True, exist_ok=True)
    log_filename = log_dir / f"update-docs_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"

    logger = logging.getLogger("TechDocFetcher")
    logger.setLevel(logging.DEBUG)

    formatter = logging.Formatter("[%(asctime)s] %(levelname)s %(message)s", datefmt="%H:%M:%S")

    file_handler = logging.FileHandler(log_filename, encoding="utf-8")
    file_handler.setFormatter(formatter)

    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setFormatter(formatter)

    logger.addHandler(file_handler)
    logger.addHandler(console_handler)

    logger.info(f"Log file: {log_filename}")
    return logger


class TechDocFetcher:
    def __init__(self, config_path: Path, logger: logging.Logger):
        self.config_path = config_path
        self.root_dir = config_path.parent.parent
        self.logger = logger
        # ë™ì‹œì„± ê·¹ë‹¨ì  ì œí•œ (Jina Rate Limit ìš°íšŒìš©)
        self.semaphore = asyncio.Semaphore(1)
        self.client = httpx.AsyncClient(
            timeout=60.0,
            follow_redirects=True,
            http2=False,  # Cloudflare Fingerprinting ë°©ì§€ (HTTP/1.1 ê°•ì œ)
            headers={
                "User-Agent": USER_AGENT,
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8",
                "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
                "Cache-Control": "no-cache",
                "Pragma": "no-cache",
            },
        )

    async def fetch_content(self, url: str) -> str:
        """Jina Readerë¥¼ ê±°ì³ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ë°ì´í„° íšë“ (ì„¸ë§ˆí¬ì–´ ë° ì¬ì‹œë„ ì ìš©)"""
        if not url:
            return ""

        target_url = f"{READER_API_PREFIX}{url}"
        async with self.semaphore:
            for attempt in range(2):
                try:
                    await asyncio.sleep(2.0 + (attempt * 2))
                    response = await self.client.get(target_url)

                    if response.status_code == 403:
                        self.logger.warning(f"[RETRY] 403 Forbidden for {url}. Falling back to urllib...")
                        return self._urllib_fallback(target_url, url)

                    response.raise_for_status()
                    return response.text

                except Exception as e:
                    self.logger.warning(f"[ERROR] Attempt {attempt + 1} failed for {url}: {e}")
                    if attempt == 1:
                        self.logger.warning(f"[FALLBACK] Trying urllib for {url}...")
                        return self._urllib_fallback(target_url, url)

        return ""

    def _urllib_fallback(self, target_url: str, original_url: str) -> str:
        """httpx ì‹¤íŒ¨ ì‹œ urllib í´ë°±"""
        try:
            req = urllib.request.Request(
                target_url,
                headers={"User-Agent": USER_AGENT, "Accept": "*/*"},
            )
            with urllib.request.urlopen(req, timeout=30) as res:
                return res.read().decode("utf-8")
        except Exception as fe:
            self.logger.error(f"[ERROR] urllib fallback failed for {original_url}: {fe}")
            return ""

    def add_front_matter(self, content: str, stack: dict, channel_type: str, url: str) -> str:
        """YAML Front-matter ë©”íƒ€ë°ì´í„° ì£¼ì… (ì‚¬ìš©ì ì •ì˜ ê·œê²©)"""
        now = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        metadata = [
            "---",
            f"Tech-Stack: {stack['name']}",
            f"Channel: {channel_type}",
            f"Source-URL: {url}",
            f"Last-Updated: {now}",
            f"Tech-Version: {stack.get('version', 'latest')}",
            f"Status: automated_pipeline",
            "---",
            "\n",
        ]
        return "\n".join(metadata) + content

    def get_hash(self, content: str) -> str:
        """ë‚´ìš© ë³€í™” ê°ì§€ë¥¼ ìœ„í•œ MD5 í•´ì‹œ ìƒì„± (ë©±ë“±ì„± ë³´ì¥)"""
        text_only = re.sub(r"\s+", "", content)
        return hashlib.md5(text_only.encode("utf-8")).hexdigest()

    def clean_content(self, content: str, channel_type: str, url: str) -> str:
        """ë¬¸ì„œ ìœ í˜•ë³„ë¡œ ë¶ˆí•„ìš”í•œ ë³´ì¼ëŸ¬í”Œë ˆì´íŠ¸(GNB, LNB, Footer)ë¥¼ ì œê±°í•˜ì—¬ í•µì‹¬ ë‚´ìš©ë§Œ ì¶”ì¶œ"""
        lines = content.splitlines()
        result = []
        
        # 1. Github Releases ì±„ë„ íŠ¹í™” (ë¦´ë¦¬ì¦ˆ ë³¸ë¬¸ ì§‘ì¤‘ ì¶”ì¶œ)
        if channel_type == "github" or "github.com" in url:
            started = False
            for line in lines:
                text = line.strip()
                # ë¦´ë¦¬ì¦ˆ íƒœê·¸ë‚˜ ë©”ì¸ í—¤ë”ê°€ ë“±ì¥í•  ë•Œë¶€í„° í´ë¦¬ë‹ ì—†ì´ ëª¨ë‘ ê¸°ë¡ ì‹œì‘
                if not started:
                    if re.match(r'^(v\d+\.|[a-zA-Z0-9_\-]+@\d+\.|Releases: |##\s+v\d+)', text):
                        started = True
                
                if started:
                    # ë§ˆí¬ë‹¤ìš´ í˜•íƒœì†Œ ì¤‘ ë¶ˆí•„ìš”í•œ GitHub UI ë‹¨ê³¨ ë¬¸êµ¬ ì œê±°
                    if "reacted with" in text or "people reacted" in text or "All reactions" in text: continue
                    if text.startswith("ğŸ‘") or text.startswith("ğŸ‰") or text.startswith("â¤ï¸") or text.startswith("ğŸš€") or text.startswith("ğŸ‘€") or text.startswith("*   ğŸ‘") or text.startswith("*   ğŸ‰") or text.startswith("*   ğŸš€"): continue
                    if "This commit was created on GitHub.com and signed" in text or "This commit was signed with the committerâ€™s" in text or "verified signature" in text: continue
                    if "GPG key ID:" in text or "SSH Key Fingerprint:" in text: continue
                    if "Learn about vigilant mode" in text or text == "Verified" or text == "Compare": continue
                    if "Choose a tag to compare" in text or "Sorry, something went wrong" in text: continue
                    if text == "Pre-release" or text == "Filter" or text == "Loading": continue
                    if "There was an error while loading." in text: continue
                    if text == "No results found" or "View all tags" in text: continue
                    if re.match(r'^\*?\s*\[([a-f0-9]{7})\]', text): continue # ì»¤ë°‹ í•´ì‹œ ë§í¬
                    if re.match(r'^!\[Image .*\]\(.*avatars\.githubusercontent\.com', text): continue # ì•„ë°”íƒ€ (ë³¸ë¶„ ì™¸)
                    if re.match(r'^\*?\s*\[v\d+\.\d+\.\d+.*\]\(.*compare/v.*', text): continue # ë²„ì „ ë¹„êµ ë¦¬ìŠ¤íŠ¸ ì „ì²´ ì‚­ì œ
                    if re.match(r'^Assets(\s+\d+)?$', text) or "Source code(zip)" in text or "Source code(tar.gz)" in text: continue
                    if text in ("=======================", "----------------------------"): continue
                    
                    result.append(line)
            
            if result:
                return "\n".join(result)
        
        # 2. PyPI (Python Registry) ì „ìš© í´ë¦¬ë‹ (ê°€ì¥ ì•…ëª… ë†’ì€ íœ  íŒŒì¼, í•´ì‹œ, ë¦´ë¦¬ì¦ˆ í­íƒ„ ì œê±°)
        if "pypi.org" in url:
            started = False
            for line in lines:
                text = line.strip()
                if text == "Project description":
                    started = True
                
                # ìš°ì¸¡ ì‚¬ì´ë“œë°” ë©”íƒ€ë°ì´í„° ë³µì œë³¸(Project details ì´í›„) ë° ë°©ëŒ€í•œ íŒŒì¼ ë¦¬ìŠ¤íŠ¸ ì°¨ë‹¨
                if text.startswith("Project details") or text.startswith("Release history") or text.startswith("Download files"):
                    break
                    
                if started:
                    if text in ("Project description", "-------------------"): continue
                    # ë¼ì¸ ì „ì²´ê°€ ë±ƒì§€ë¡œë§Œ ì´ë£¨ì–´ì§„ ê²½ìš° ìŠ¤í‚µ
                    if re.match(r'^(!?\[Image [^\]]+\]\([^)]+\))+\s*$', text): continue
                    result.append(line)
            
            if result:
                return "\n".join(result)

        # 3. Crates.io (Rust Registry) íŠ¹í™”
        if "crates.io" in url:
            for line in lines:
                text = line.strip()
                # ì¸ë¼ì¸ ë±ƒì§€ë“¤ ì¼ê´„ ì œê±°
                line_no_badges = re.sub(r'!?\[Image [^\]]+\]\([^)]+\)', '', line)
                line_no_badges_link = re.sub(r'\[(!?\[Image [^\]]+\]\([^)]+\))\]\([^)]+\)', '', line)
                if len(line_no_badges.strip()) == 0 or len(line_no_badges_link.strip()) == 0:
                    continue
                if text.startswith("| Component | Version |") or text.startswith("| --- | --- |"): continue
                if re.match(r'^\|\s*tauri\s*\|\s*!\[Image', text): continue
                result.append(line)
                
            if result:
                return "\n".join(result)

        # 4. ë²”ìš© ì›¹ì‚¬ì´íŠ¸ í´ë¦¬ë‹ (Official, Registry ê³µí†µ)
        skip_phrases = [
            "Skip to content", "Navigation Menu", "Toggle navigation", 
            "Sign in", "Sign up", "Search or jump to", 
            "Provide feedback", "We read every piece of feedback",
            "Appearance settings", "Security Update: Classic tokens",
            "package search", "Readme Code Beta", "Dependencies", "Dependents"
        ]
        
        # npm registry ë“± ë²”ìš©ì—ì„œ í”íˆ ë³´ì´ëŠ” Header, Footer ì œê±°ìš© ìƒíƒœ ë³€ìˆ˜
        in_footer = False
        
        for line in lines:
            text = line.strip()
            if text == "Footer" or text.startswith("Footer navigation") or text == "Terms & Policies":
                in_footer = True
            
            if in_footer:
                continue

            if any(phrase in text for phrase in skip_phrases) and len(text) < 120:
                continue
                
            # NPM íƒ­ ë©”ë‰´ ë° ìì˜í•œ UI í…ìŠ¤íŠ¸ ì œê±°
            if re.match(r'^\*\s+\[(Readme|Code Beta|\d+ Dependencies|\d+ Dependents|[\d,]+ Versions)\]', text): continue
            
            result.append(line)
            
        # ì—°ì†ëœ ë¹ˆ ì¤„(3ì¤„ ì´ìƒ) ë° êµ¬ë¶„ì„  ì••ì¶•. ê³µë°±ë¬¸ìê°€ ì„ì¸ ê²½ìš°ë„ í¬ê´„í•˜ì—¬ íŒŒê´´ì ì¸ ê³µë°± ì••ì¶•
        cleaned_text = "\n".join(result)
        cleaned_text = re.sub(r'\n[ \t]*\n([ \t]*\n)+', '\n\n', cleaned_text)
        return cleaned_text.strip()

    async def process_channel(self, stack: dict, channel_type: str, url: str) -> str:
        """ë‹¨ì¼ ì±„ë„ ìˆ˜ì§‘ ë° ì¦ë¶„ ì—…ë°ì´íŠ¸. ê²°ê³¼ ìƒíƒœ ë¬¸ìì—´ ë°˜í™˜: 'updated' | 'skipped' | 'failed'"""
        target_dir = self.root_dir / stack["target_dir"]
        target_filename = f"{stack['name']}-{channel_type}.md"
        target_path = target_dir / target_filename

        # 1. ì½˜í…ì¸  íšë“
        raw_md = await self.fetch_content(url)
        if not raw_md or len(raw_md.strip()) < 100:
            self.logger.warning(f"[FAILED] {stack['name']} ({channel_type}): empty or too short content.")
            return "failed"

        # 1.5. ë³¸ë¬¸ í•µì‹¬ ìš”ì†Œ ì¶”ì¶œ (ë³´ì¼ëŸ¬í”Œë ˆì´íŠ¸ ì œê±° íŒŒì´í”„ë¼ì¸)
        cleaned_md = self.clean_content(raw_md, channel_type, url)
        # í•„í„°ë§ ë¶€ì‘ìš©ìœ¼ë¡œ ë³¸ë¬¸ì´ ë‚ ì•„ê°„ ê²½ìš° ì›ë³¸ ë³µêµ¬ (ì•ˆì „ë§)
        if len(cleaned_md) < 200:
            cleaned_md = raw_md

        current_hash = self.get_hash(cleaned_md)

        # 2. ì¦ë¶„ ì—…ë°ì´íŠ¸ í™•ì¸ (Fingerprint ê¸°ë°˜)
        if target_path.exists():
            existing_text = target_path.read_text(encoding="utf-8")
            if f"Fingerprint: {current_hash}" in existing_text:
                self.logger.info(f"[-] No changes: {stack['name']} ({channel_type}). Skipping.")
                return "skipped"

        # 3. ë¬¸ì„œ ê²°í•© ë° ì €ì¥
        full_content = self.add_front_matter(cleaned_md, stack, channel_type, url)
        full_content += f"\n\n---\n*Fingerprint: {current_hash}*"

        target_path.parent.mkdir(parents=True, exist_ok=True)
        target_path.write_text(full_content, encoding="utf-8")
        self.logger.info(f"[+] Updated: {target_path}")
        return "updated"

    async def run(self, filter_stacks: list[str] | None = None, filter_channels: list[str] | None = None):
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ ê°€ë™"""
        if not self.config_path.exists():
            self.logger.error(f"Configuration file not found: {self.config_path}")
            return

        config = json.loads(self.config_path.read_text(encoding="utf-8"))

        tasks = []
        task_labels = []
        for stack in config.get("tech_stacks", []):
            if filter_stacks and stack["name"] not in filter_stacks:
                continue
            channels = stack.get("channels", {})
            for ch_type, url in channels.items():
                if filter_channels and ch_type not in filter_channels:
                    continue
                if url:
                    tasks.append(self.process_channel(stack, ch_type, url))
                    task_labels.append(f"{stack['name']}/{ch_type}")

        if not tasks:
            self.logger.warning("[!] No matching URLs found to update.")
            await self.client.aclose()
            return

        self.logger.info(f"[*] Starting update for {len(tasks)} documentation sources...")
        results = await asyncio.gather(*tasks)

        # ê²°ê³¼ ìš”ì•½
        counts = {"updated": 0, "skipped": 0, "failed": 0}
        for label, result in zip(task_labels, results):
            counts[result] += 1

        self.logger.info("=" * 54)
        self.logger.info(f"[SUMMARY] Total: {len(tasks)} | "
                         f"Updated: {counts['updated']} | "
                         f"Skipped: {counts['skipped']} | "
                         f"Failed: {counts['failed']}")
        if counts["failed"] > 0:
            failed_labels = [label for label, r in zip(task_labels, results) if r == "failed"]
            self.logger.warning(f"[FAILED CHANNELS] {', '.join(failed_labels)}")
        self.logger.info("=" * 54)

        await self.client.aclose()


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Tech Stack Organizer - Documentation Fetcher")
    parser.add_argument("--stacks", nargs="+", metavar="STACK",
                        help="Update only specific stacks (e.g. --stacks python nuitka)")
    parser.add_argument("--channels", nargs="+", metavar="CHANNEL",
                        help="Update only specific channels (e.g. --channels official github)")
    args = parser.parse_args()

    # Windows í™˜ê²½ì—ì„œì˜ ë¹„ë™ê¸° ì •ì±… ì„¤ì •
    if os.name == "nt":
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())

    # í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê¸°ë°˜ ì ˆëŒ€ê²½ë¡œ ì„¤ì •
    base_dir = Path(__file__).resolve().parent.parent.parent
    config_file = base_dir / "config" / "sources.json"
    log_dir = base_dir / "logs"

    logger = setup_logging(log_dir)
    fetcher = TechDocFetcher(config_file, logger)
    asyncio.run(fetcher.run(filter_stacks=args.stacks, filter_channels=args.channels))
